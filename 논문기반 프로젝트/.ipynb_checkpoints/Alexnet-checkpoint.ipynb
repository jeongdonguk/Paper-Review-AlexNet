{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "167136e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "674a846f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30ae3010",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './../../data/식물잎/splitted/'\n",
    "train_dir = os.path.join(data_dir,'train')\n",
    "val_dir = os.path.join(data_dir,'val')\n",
    "test_dir = os.path.join(data_dir,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "504817fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'image_size' : 256,\n",
    "    'epochs' : 100,\n",
    "    'lr' : 1e-3,\n",
    "    'batch_size' : 128,\n",
    "    'seed' :2023\n",
    "}\n",
    "\n",
    "np.random.seed(cfg['seed'])\n",
    "torch.manual_seed(cfg['seed'])\n",
    "torch.cuda.manual_seed_all(cfg['seed'])\n",
    "\n",
    "data_transforms = {\n",
    "    'train' : transforms.Compose([\n",
    "        transforms.Resize([cfg['image_size'],cfg['image_size']]),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'val' : transforms.Compose([\n",
    "        transforms.Resize([cfg['image_size'],cfg['image_size']]),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'test' : transforms.Compose([\n",
    "        transforms.Resize([cfg['image_size'],cfg['image_size']]),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "}\n",
    "\n",
    "\n",
    "train_ds = ImageFolder(root=train_dir, transform=data_transforms['train'])\n",
    "val_ds = ImageFolder(root=val_dir, transform=data_transforms['val'])\n",
    "test_ds = ImageFolder(root=test_dir, transform=data_transforms['test'])\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg['batch_size'], shuffle=True, num_workers=3)\n",
    "val_loader = DataLoader(val_ds, batch_size=cfg['batch_size'], shuffle=True, num_workers=3)\n",
    "test_loader = DataLoader(test_ds, batch_size=cfg['batch_size'], shuffle=True, num_workers=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "146cab5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 3, 224, 224])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d92ad187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0.5451, 0.6588, 0.4627,  ..., 0.4784, 0.5333, 0.5843],\n",
      "          [0.5804, 0.5451, 0.6118,  ..., 0.4980, 0.5686, 0.6667],\n",
      "          [0.5961, 0.5490, 0.4863,  ..., 0.5137, 0.4863, 0.5216],\n",
      "          ...,\n",
      "          [0.8863, 0.9098, 0.8863,  ..., 0.6941, 0.6510, 0.6078],\n",
      "          [0.8902, 0.8941, 0.8863,  ..., 0.6980, 0.5922, 0.6510],\n",
      "          [0.8784, 0.8549, 0.8627,  ..., 0.6706, 0.7176, 0.5804]],\n",
      "\n",
      "         [[0.4902, 0.6039, 0.4078,  ..., 0.4314, 0.4863, 0.5373],\n",
      "          [0.5255, 0.4902, 0.5569,  ..., 0.4510, 0.5216, 0.6196],\n",
      "          [0.5412, 0.4941, 0.4314,  ..., 0.4667, 0.4392, 0.4745],\n",
      "          ...,\n",
      "          [0.8706, 0.8941, 0.8824,  ..., 0.6784, 0.6314, 0.5882],\n",
      "          [0.8667, 0.8784, 0.8706,  ..., 0.6824, 0.5725, 0.6314],\n",
      "          [0.8549, 0.8314, 0.8392,  ..., 0.6549, 0.6980, 0.5608]],\n",
      "\n",
      "         [[0.4863, 0.6000, 0.4039,  ..., 0.4392, 0.4941, 0.5451],\n",
      "          [0.5216, 0.4863, 0.5529,  ..., 0.4588, 0.5294, 0.6275],\n",
      "          [0.5373, 0.4902, 0.4275,  ..., 0.4745, 0.4471, 0.4824],\n",
      "          ...,\n",
      "          [0.8745, 0.8902, 0.8745,  ..., 0.6824, 0.6471, 0.6039],\n",
      "          [0.8745, 0.8745, 0.8667,  ..., 0.6863, 0.5882, 0.6471],\n",
      "          [0.8627, 0.8314, 0.8392,  ..., 0.6588, 0.7137, 0.5765]]],\n",
      "\n",
      "\n",
      "        [[[0.3176, 0.3137, 0.3059,  ..., 0.2745, 0.2784, 0.2902],\n",
      "          [0.3333, 0.3294, 0.3137,  ..., 0.2706, 0.2706, 0.2745],\n",
      "          [0.3098, 0.3176, 0.3137,  ..., 0.2667, 0.2667, 0.2706],\n",
      "          ...,\n",
      "          [0.4902, 0.4941, 0.4902,  ..., 0.4314, 0.4392, 0.4588],\n",
      "          [0.4941, 0.4980, 0.4941,  ..., 0.4431, 0.4510, 0.4627],\n",
      "          [0.4980, 0.5059, 0.4980,  ..., 0.4471, 0.4471, 0.4510]],\n",
      "\n",
      "         [[0.3020, 0.2980, 0.2902,  ..., 0.2588, 0.2627, 0.2745],\n",
      "          [0.3176, 0.3137, 0.2980,  ..., 0.2549, 0.2549, 0.2588],\n",
      "          [0.2941, 0.3020, 0.2980,  ..., 0.2510, 0.2510, 0.2549],\n",
      "          ...,\n",
      "          [0.4863, 0.4902, 0.4863,  ..., 0.4314, 0.4392, 0.4588],\n",
      "          [0.4902, 0.4941, 0.4902,  ..., 0.4431, 0.4510, 0.4627],\n",
      "          [0.4941, 0.5020, 0.4941,  ..., 0.4471, 0.4471, 0.4510]],\n",
      "\n",
      "         [[0.2588, 0.2549, 0.2471,  ..., 0.2157, 0.2196, 0.2314],\n",
      "          [0.2745, 0.2706, 0.2549,  ..., 0.2118, 0.2118, 0.2157],\n",
      "          [0.2510, 0.2588, 0.2549,  ..., 0.2078, 0.2078, 0.2118],\n",
      "          ...,\n",
      "          [0.4706, 0.4745, 0.4706,  ..., 0.4000, 0.4078, 0.4275],\n",
      "          [0.4745, 0.4784, 0.4745,  ..., 0.4118, 0.4196, 0.4314],\n",
      "          [0.4784, 0.4863, 0.4784,  ..., 0.4157, 0.4157, 0.4196]]],\n",
      "\n",
      "\n",
      "        [[[0.3843, 0.3451, 0.3059,  ..., 0.4941, 0.5098, 0.5059],\n",
      "          [0.3608, 0.3882, 0.3451,  ..., 0.5059, 0.5020, 0.4863],\n",
      "          [0.3765, 0.4549, 0.3725,  ..., 0.5059, 0.4980, 0.4863],\n",
      "          ...,\n",
      "          [0.6235, 0.6392, 0.5098,  ..., 0.5294, 0.5255, 0.5176],\n",
      "          [0.5216, 0.5137, 0.5922,  ..., 0.5176, 0.5137, 0.5020],\n",
      "          [0.5804, 0.5804, 0.5647,  ..., 0.5020, 0.4941, 0.4784]],\n",
      "\n",
      "         [[0.3176, 0.2784, 0.2392,  ..., 0.4275, 0.4431, 0.4392],\n",
      "          [0.2941, 0.3216, 0.2784,  ..., 0.4392, 0.4353, 0.4196],\n",
      "          [0.3098, 0.3882, 0.3059,  ..., 0.4392, 0.4314, 0.4196],\n",
      "          ...,\n",
      "          [0.5647, 0.5804, 0.4588,  ..., 0.4902, 0.4863, 0.4784],\n",
      "          [0.4627, 0.4549, 0.5412,  ..., 0.4784, 0.4745, 0.4627],\n",
      "          [0.5216, 0.5216, 0.5137,  ..., 0.4627, 0.4549, 0.4392]],\n",
      "\n",
      "         [[0.3882, 0.3490, 0.3098,  ..., 0.5020, 0.5176, 0.5137],\n",
      "          [0.3647, 0.3922, 0.3490,  ..., 0.5137, 0.5098, 0.4941],\n",
      "          [0.3804, 0.4588, 0.3765,  ..., 0.5137, 0.5059, 0.4941],\n",
      "          ...,\n",
      "          [0.6392, 0.6549, 0.5294,  ..., 0.5843, 0.5804, 0.5725],\n",
      "          [0.5373, 0.5294, 0.6118,  ..., 0.5725, 0.5686, 0.5569],\n",
      "          [0.5961, 0.5961, 0.5843,  ..., 0.5569, 0.5490, 0.5333]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.8235, 0.8314, 0.8392,  ..., 0.6588, 0.6510, 0.6353],\n",
      "          [0.8314, 0.8275, 0.8314,  ..., 0.6549, 0.6588, 0.6510],\n",
      "          [0.8275, 0.8196, 0.8196,  ..., 0.6157, 0.6431, 0.6588],\n",
      "          ...,\n",
      "          [0.6471, 0.7216, 0.7098,  ..., 0.5686, 0.5569, 0.5294],\n",
      "          [0.7098, 0.7137, 0.6941,  ..., 0.5961, 0.6392, 0.5765],\n",
      "          [0.6980, 0.7059, 0.6941,  ..., 0.5843, 0.5529, 0.6078]],\n",
      "\n",
      "         [[0.8078, 0.8157, 0.8235,  ..., 0.5961, 0.5882, 0.5647],\n",
      "          [0.8157, 0.8118, 0.8157,  ..., 0.5922, 0.5961, 0.5804],\n",
      "          [0.8118, 0.8039, 0.8039,  ..., 0.5529, 0.5804, 0.5882],\n",
      "          ...,\n",
      "          [0.6235, 0.6980, 0.6863,  ..., 0.4941, 0.4824, 0.4510],\n",
      "          [0.6863, 0.6902, 0.6706,  ..., 0.5216, 0.5647, 0.4980],\n",
      "          [0.6745, 0.6824, 0.6706,  ..., 0.5098, 0.4784, 0.5294]],\n",
      "\n",
      "         [[0.8039, 0.8118, 0.8196,  ..., 0.6000, 0.5922, 0.5647],\n",
      "          [0.8118, 0.8078, 0.8118,  ..., 0.5961, 0.6000, 0.5804],\n",
      "          [0.8078, 0.8000, 0.8000,  ..., 0.5569, 0.5843, 0.5882],\n",
      "          ...,\n",
      "          [0.6314, 0.7059, 0.6941,  ..., 0.5020, 0.4902, 0.4588],\n",
      "          [0.6941, 0.6980, 0.6784,  ..., 0.5294, 0.5725, 0.5059],\n",
      "          [0.6824, 0.6902, 0.6784,  ..., 0.5176, 0.4863, 0.5373]]],\n",
      "\n",
      "\n",
      "        [[[0.6824, 0.6549, 0.6510,  ..., 0.5843, 0.5922, 0.6000],\n",
      "          [0.6784, 0.6627, 0.6667,  ..., 0.5686, 0.5725, 0.5843],\n",
      "          [0.7020, 0.6667, 0.6784,  ..., 0.5725, 0.5686, 0.5804],\n",
      "          ...,\n",
      "          [0.3412, 0.2980, 0.2784,  ..., 0.4902, 0.4902, 0.4824],\n",
      "          [0.3765, 0.3333, 0.2784,  ..., 0.4902, 0.4824, 0.4745],\n",
      "          [0.4118, 0.3961, 0.3059,  ..., 0.5098, 0.5137, 0.5137]],\n",
      "\n",
      "         [[0.6510, 0.6235, 0.6196,  ..., 0.5569, 0.5647, 0.5725],\n",
      "          [0.6471, 0.6314, 0.6353,  ..., 0.5412, 0.5451, 0.5569],\n",
      "          [0.6706, 0.6353, 0.6471,  ..., 0.5451, 0.5412, 0.5529],\n",
      "          ...,\n",
      "          [0.3569, 0.3451, 0.3373,  ..., 0.4157, 0.4157, 0.4078],\n",
      "          [0.3922, 0.3686, 0.3333,  ..., 0.4157, 0.4078, 0.4000],\n",
      "          [0.4196, 0.4314, 0.3608,  ..., 0.4353, 0.4392, 0.4392]],\n",
      "\n",
      "         [[0.7098, 0.6824, 0.6784,  ..., 0.6275, 0.6353, 0.6431],\n",
      "          [0.7059, 0.6902, 0.6941,  ..., 0.6118, 0.6157, 0.6275],\n",
      "          [0.7294, 0.6941, 0.7059,  ..., 0.6157, 0.6118, 0.6235],\n",
      "          ...,\n",
      "          [0.2510, 0.1647, 0.1059,  ..., 0.4902, 0.4902, 0.4824],\n",
      "          [0.2941, 0.2000, 0.1020,  ..., 0.4902, 0.4824, 0.4745],\n",
      "          [0.3255, 0.2627, 0.1294,  ..., 0.5098, 0.5137, 0.5137]]],\n",
      "\n",
      "\n",
      "        [[[0.2157, 0.2196, 0.2196,  ..., 0.0431, 0.0471, 0.0549],\n",
      "          [0.2118, 0.2235, 0.2314,  ..., 0.0392, 0.0510, 0.0667],\n",
      "          [0.2039, 0.2157, 0.2275,  ..., 0.0431, 0.0549, 0.0706],\n",
      "          ...,\n",
      "          [0.3608, 0.3882, 0.3725,  ..., 0.1843, 0.1647, 0.1373],\n",
      "          [0.3647, 0.3529, 0.3216,  ..., 0.1373, 0.1294, 0.1098],\n",
      "          [0.3569, 0.3412, 0.3255,  ..., 0.1373, 0.1294, 0.1098]],\n",
      "\n",
      "         [[0.3529, 0.3569, 0.3569,  ..., 0.1608, 0.1647, 0.1725],\n",
      "          [0.3490, 0.3608, 0.3686,  ..., 0.1569, 0.1686, 0.1843],\n",
      "          [0.3490, 0.3608, 0.3765,  ..., 0.1608, 0.1725, 0.1882],\n",
      "          ...,\n",
      "          [0.5137, 0.5412, 0.5255,  ..., 0.1412, 0.1216, 0.0980],\n",
      "          [0.5176, 0.5059, 0.4745,  ..., 0.1412, 0.1216, 0.1020],\n",
      "          [0.5098, 0.4941, 0.4784,  ..., 0.1412, 0.1216, 0.1020]],\n",
      "\n",
      "         [[0.1412, 0.1451, 0.1373,  ..., 0.0118, 0.0157, 0.0235],\n",
      "          [0.1373, 0.1490, 0.1490,  ..., 0.0078, 0.0196, 0.0353],\n",
      "          [0.1373, 0.1490, 0.1529,  ..., 0.0118, 0.0235, 0.0392],\n",
      "          ...,\n",
      "          [0.3373, 0.3647, 0.3490,  ..., 0.1255, 0.1137, 0.0902],\n",
      "          [0.3333, 0.3216, 0.2902,  ..., 0.0863, 0.0706, 0.0510],\n",
      "          [0.3255, 0.3098, 0.2941,  ..., 0.0863, 0.0706, 0.0510]]]]), tensor([11, 30, 27,  6, 28,  5,  4, 14,  6, 12, 25, 28, 29, 25, 26, 17, 14, 21,\n",
      "        19, 12, 28, 23,  4,  7,  7, 25, 25, 32, 32,  6, 25,  5,  4,  4, 30, 32,\n",
      "        14, 24, 21, 23, 15, 28, 27,  1, 14,  9, 19, 23, 28, 22, 17, 28, 25, 23,\n",
      "         5,  9,  3, 14, 30, 30,  5, 12, 16, 30,  3, 27, 22, 11, 23, 30, 23, 10,\n",
      "        31,  4,  1, 19, 14, 13, 11, 19, 25, 12,  3, 20, 30, 11, 24,  9, 21,  2,\n",
      "        11, 25,  1, 24, 27, 21, 27,  8, 13,  5,  3, 21, 30, 30, 12, 12, 16, 16,\n",
      "        10, 30, 26,  5, 14, 18,  1,  6, 14,  3,  7, 22, 14, 22, 10, 21, 14, 11,\n",
      "        19,  9])]\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a15cf3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class alexnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(alexnet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=96)\n",
    "        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=2, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=256)\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=4, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=384)\n",
    "        self.conv4 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=4, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=384)\n",
    "        self.conv5 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=4, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=256)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=256*6*6, out_features=500)\n",
    "        self.fc2 = nn.Linear(in_features=500, out_features=200)\n",
    "        self.fc3 = nn.Linear(in_features=200, out_features=33)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 1 layer\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        \n",
    "        # 2 layer\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        \n",
    "        # 3 layer\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # 4 layer\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # 5 layer\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        # 6 layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5)\n",
    "\n",
    "        # 7 layer\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, p=0.5)\n",
    "        \n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4ac74a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet_model = alexnet().to(device)\n",
    "optimizer = torch.optim.Adam(alexnet_model.parameters(), lr = cfg['lr'])\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "de635152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f27972ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [96, 3, 11, 11], expected input[2, 1, 227, 227] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [77]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43malexnet_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m227\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m227\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchsummary\\torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [72]\u001b[0m, in \u001b[0;36malexnet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# 1 layer\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[0;32m     24\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1148\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks)\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m-> 1148\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    451\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    452\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [96, 3, 11, 11], expected input[2, 1, 227, 227] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "summary(alexnet_model, (1, 227, 227))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf754892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model=alexnet_model, train_loader=train_loader,\n",
    "          optimizer=optimizer, lr_scheduler=lr_scheduler):\n",
    "\n",
    "    model.train()\n",
    "    for data, label in train_loader:\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        loss = F.cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7edf3a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model=alexnet_model, val_loader=val_loader,\n",
    "            optimizer=optimizer, lr_scheduler=lr_scheduler):\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in val_loader:\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            test_loss += F.cross_entropy(output, label, reduction='sum').item()\n",
    "            \n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            test_acc += pred.eq(label.view_as(pred)).sum().item()\n",
    "        \n",
    "    test_loss /= len(val_loader.dataset)\n",
    "    test_acc = 100*test_acc/len(val_loader.dataset)\n",
    "    \n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf81943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model = alexnet_model, train_loader=train_loader, val_loader =val_loader,\n",
    "               optimizer =optimizer, lr_scheduler = lr_scheduler , epochs = cfg['epochs']):\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    stop_num = 0\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        since = time.time()\n",
    "        train()\n",
    "        train_loss, train_acc = evaluate(model, train_loader)\n",
    "        val_loss, val_acc = evaluate(model, val_loader)\n",
    "        lr_scheduler.step(val_loss)\n",
    "        \n",
    "        if val_acc > best_acc :\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            stop_num = 0\n",
    "        else :\n",
    "            stop_num +=1\n",
    "        \n",
    "        end_time = time.time() - since\n",
    "        \n",
    "        print(f'--------------{i+1}-----------')\n",
    "        print(f'train loss : {round(train_loss,3)},  acc : {round(train_acc,3)}%')\n",
    "        print(f'  val loss : {round(train_loss,3)},  acc : {round(train_acc,3)}%')\n",
    "        print(f'걸린시간 : {end_time}초')\n",
    "        \n",
    "        if stop_num >=10:\n",
    "            print('조기종료')\n",
    "            break\n",
    "        \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b49e752",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (256x1x1). Calculated output size: (256x0x0). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, optimizer, lr_scheduler, epochs)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      9\u001b[0m     since \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 10\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m evaluate(model, train_loader)\n\u001b[0;32m     12\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader)\n",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, optimizer, lr_scheduler)\u001b[0m\n\u001b[0;32m      6\u001b[0m data, label \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), label\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 8\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m _, pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(output, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(output, label)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36malexnet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     45\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn5(x)\n\u001b[0;32m     46\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m---> 47\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# 6 layer\u001b[39;00m\n\u001b[0;32m     50\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_jit_internal.py:423\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 423\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:782\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    781\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 782\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given input size: (256x1x1). Calculated output size: (256x0x0). Output size is too small"
     ]
    }
   ],
   "source": [
    "trained_model = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1370c28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
