{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "167136e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "674a846f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30ae3010",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './../../data/식물잎/splitted/'\n",
    "train_dir = os.path.join(data_dir,'train')\n",
    "val_dir = os.path.join(data_dir,'val')\n",
    "test_dir = os.path.join(data_dir,'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "504817fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'image_size' : 235,\n",
    "    'epochs' : 200,\n",
    "    'lr' : 0.5e-3,\n",
    "    'batch_size' : 128,\n",
    "    'seed' :2023\n",
    "}\n",
    "\n",
    "np.random.seed(cfg['seed'])\n",
    "torch.manual_seed(cfg['seed'])\n",
    "torch.cuda.manual_seed_all(cfg['seed'])\n",
    "\n",
    "data_transforms = {\n",
    "    'train' : transforms.Compose([\n",
    "        transforms.Resize([cfg['image_size'],cfg['image_size']]),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomCrop(224,padding=4),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'val' : transforms.Compose([\n",
    "        transforms.Resize([cfg['image_size'],cfg['image_size']]),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'test' : transforms.Compose([\n",
    "        transforms.Resize([cfg['image_size'],cfg['image_size']]),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "}\n",
    "\n",
    "\n",
    "train_ds = ImageFolder(root=train_dir, transform=data_transforms['train'])\n",
    "val_ds = ImageFolder(root=val_dir, transform=data_transforms['val'])\n",
    "test_ds = ImageFolder(root=test_dir, transform=data_transforms['test'])\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg['batch_size'], shuffle=True, num_workers=3)\n",
    "val_loader = DataLoader(val_ds, batch_size=cfg['batch_size'], shuffle=True, num_workers=3)\n",
    "test_loader = DataLoader(test_ds, batch_size=cfg['batch_size'], shuffle=True, num_workers=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d92ad187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0.5176, 0.6902, 0.6745,  ..., 0.4549, 0.0000, 0.0000],\n",
      "          [0.5922, 0.6706, 0.6549,  ..., 0.4667, 0.0000, 0.0000],\n",
      "          [0.5922, 0.6275, 0.6314,  ..., 0.4431, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.8824, 0.8863, 0.8824,  ..., 0.6627, 0.0000, 0.0000],\n",
      "          [0.8706, 0.8745, 0.8824,  ..., 0.6431, 0.0000, 0.0000],\n",
      "          [0.8667, 0.8706, 0.8706,  ..., 0.6471, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4745, 0.6431, 0.6196,  ..., 0.4078, 0.0000, 0.0000],\n",
      "          [0.5490, 0.6235, 0.6000,  ..., 0.4196, 0.0000, 0.0000],\n",
      "          [0.5490, 0.5843, 0.5765,  ..., 0.3961, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.8784, 0.8824, 0.8784,  ..., 0.6431, 0.0000, 0.0000],\n",
      "          [0.8667, 0.8706, 0.8784,  ..., 0.6235, 0.0000, 0.0000],\n",
      "          [0.8627, 0.8667, 0.8667,  ..., 0.6275, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.4667, 0.6353, 0.6157,  ..., 0.4157, 0.0000, 0.0000],\n",
      "          [0.5412, 0.6196, 0.5961,  ..., 0.4275, 0.0000, 0.0000],\n",
      "          [0.5412, 0.5765, 0.5725,  ..., 0.4039, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.8706, 0.8745, 0.8706,  ..., 0.6588, 0.0000, 0.0000],\n",
      "          [0.8588, 0.8627, 0.8706,  ..., 0.6392, 0.0000, 0.0000],\n",
      "          [0.8549, 0.8588, 0.8588,  ..., 0.6431, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.2902, 0.2863, 0.2863,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.2941, 0.2902, 0.2902,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.3098, 0.2941, 0.2902,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.4941, 0.4941, 0.4863,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.4902, 0.4902, 0.4863,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.4784, 0.4824, 0.4784,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.2745, 0.2706, 0.2706,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.2745, 0.2706, 0.2706,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.2902, 0.2745, 0.2706,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.4902, 0.4902, 0.4824,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.4863, 0.4863, 0.4824,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.4745, 0.4784, 0.4745,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.2392, 0.2353, 0.2353,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.2510, 0.2471, 0.2471,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.2667, 0.2510, 0.2471,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.4745, 0.4745, 0.4667,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.4706, 0.4706, 0.4667,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.4588, 0.4627, 0.4588,  ..., 0.0000, 0.0000, 0.0000]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000,  ..., 0.4745, 0.4588, 0.4588],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.4510, 0.4431, 0.4549],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.4353, 0.4275, 0.4471],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.5176, 0.5176, 0.5020],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.5255, 0.5137, 0.5020],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.5255, 0.5176, 0.5176]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.4157, 0.4000, 0.4000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.3922, 0.3843, 0.3961],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.3765, 0.3686, 0.3882],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.4784, 0.4784, 0.4627],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.4863, 0.4745, 0.4627],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.4863, 0.4784, 0.4784]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.4902, 0.4745, 0.4745],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.4667, 0.4588, 0.4706],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.4510, 0.4431, 0.4627],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.5725, 0.5725, 0.5569],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.5804, 0.5686, 0.5569],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.5804, 0.5725, 0.5725]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000,  ..., 0.6431, 0.6549, 0.6471],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.6196, 0.6275, 0.6471],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.6078, 0.6196, 0.6314],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.6000, 0.5922, 0.5961],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.6118, 0.6039, 0.5725],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.6078, 0.6000, 0.5529]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.5804, 0.5922, 0.5843],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.5569, 0.5647, 0.5843],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.5451, 0.5569, 0.5686],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.5216, 0.5137, 0.5176],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.5333, 0.5255, 0.4941],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.5294, 0.5216, 0.4745]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.5804, 0.5922, 0.5843],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.5569, 0.5647, 0.5882],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.5451, 0.5569, 0.5725],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.5294, 0.5216, 0.5255],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.5412, 0.5333, 0.5020],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.5373, 0.5294, 0.4824]]],\n",
      "\n",
      "\n",
      "        [[[0.6314, 0.6353, 0.6471,  ..., 0.5765, 0.5686, 0.5843],\n",
      "          [0.6235, 0.6353, 0.6588,  ..., 0.5961, 0.5882, 0.5765],\n",
      "          [0.6431, 0.6471, 0.6667,  ..., 0.6157, 0.6000, 0.5608],\n",
      "          ...,\n",
      "          [0.4941, 0.5059, 0.4980,  ..., 0.5020, 0.4902, 0.4667],\n",
      "          [0.4863, 0.4824, 0.4863,  ..., 0.5020, 0.4863, 0.4627],\n",
      "          [0.4863, 0.4627, 0.4784,  ..., 0.4824, 0.4706, 0.4510]],\n",
      "\n",
      "         [[0.6000, 0.6039, 0.6157,  ..., 0.5490, 0.5412, 0.5569],\n",
      "          [0.5922, 0.6039, 0.6275,  ..., 0.5686, 0.5608, 0.5490],\n",
      "          [0.6118, 0.6157, 0.6353,  ..., 0.5882, 0.5725, 0.5333],\n",
      "          ...,\n",
      "          [0.4078, 0.4196, 0.4118,  ..., 0.4196, 0.4078, 0.3843],\n",
      "          [0.4000, 0.3961, 0.4000,  ..., 0.4196, 0.4039, 0.3804],\n",
      "          [0.4000, 0.3765, 0.3922,  ..., 0.4000, 0.3882, 0.3686]],\n",
      "\n",
      "         [[0.6588, 0.6627, 0.6745,  ..., 0.6196, 0.6118, 0.6275],\n",
      "          [0.6510, 0.6627, 0.6863,  ..., 0.6392, 0.6314, 0.6196],\n",
      "          [0.6706, 0.6745, 0.6941,  ..., 0.6588, 0.6431, 0.6039],\n",
      "          ...,\n",
      "          [0.4549, 0.4667, 0.4588,  ..., 0.4863, 0.4745, 0.4510],\n",
      "          [0.4471, 0.4431, 0.4471,  ..., 0.4863, 0.4706, 0.4471],\n",
      "          [0.4471, 0.4235, 0.4392,  ..., 0.4667, 0.4549, 0.4353]]],\n",
      "\n",
      "\n",
      "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.5137,  ..., 0.2745, 0.2784, 0.2471],\n",
      "          [0.0000, 0.0000, 0.5333,  ..., 0.2902, 0.2784, 0.2667],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.3176,  ..., 0.2588, 0.2627, 0.2510],\n",
      "          [0.0000, 0.0000, 0.3569,  ..., 0.2980, 0.2235, 0.1725],\n",
      "          [0.0000, 0.0000, 0.3922,  ..., 0.2941, 0.1686, 0.1294]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.3765,  ..., 0.4314, 0.4078, 0.3647],\n",
      "          [0.0000, 0.0000, 0.3961,  ..., 0.4471, 0.4078, 0.3843],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.4627,  ..., 0.4157, 0.3412, 0.2902],\n",
      "          [0.0000, 0.0000, 0.4980,  ..., 0.4353, 0.2745, 0.1804],\n",
      "          [0.0000, 0.0000, 0.5333,  ..., 0.4157, 0.2157, 0.1373]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.2588,  ..., 0.1882, 0.2000, 0.1725],\n",
      "          [0.0000, 0.0000, 0.2784,  ..., 0.2039, 0.2000, 0.1843],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.3020,  ..., 0.2000, 0.1922, 0.1843],\n",
      "          [0.0000, 0.0000, 0.3490,  ..., 0.2392, 0.1725, 0.1255],\n",
      "          [0.0000, 0.0000, 0.3843,  ..., 0.2314, 0.1176, 0.0824]]]]), tensor([11, 30, 27,  6, 28,  5,  4, 14,  6, 12, 25, 28, 29, 25, 26, 17, 14, 21,\n",
      "        19, 12, 28, 23,  4,  7,  7, 25, 25, 32, 32,  6, 25,  5,  4,  4, 30, 32,\n",
      "        14, 24, 21, 23, 15, 28, 27,  1, 14,  9, 19, 23, 28, 22, 17, 28, 25, 23,\n",
      "         5,  9,  3, 14, 30, 30,  5, 12, 16, 30,  3, 27, 22, 11, 23, 30, 23, 10,\n",
      "        31,  4,  1, 19, 14, 13, 11, 19, 25, 12,  3, 20, 30, 11, 24,  9, 21,  2,\n",
      "        11, 25,  1, 24, 27, 21, 27,  8, 13,  5,  3, 21, 30, 30, 12, 12, 16, 16,\n",
      "        10, 30, 26,  5, 14, 18,  1,  6, 14,  3,  7, 22, 14, 22, 10, 21, 14, 11,\n",
      "        19,  9])]\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a15cf3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class alexnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(alexnet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=96)\n",
    "        self.conv2 = nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=256)\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=384)\n",
    "        self.conv4 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=384)\n",
    "        self.conv5 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=256)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=256*6*6, out_features=500)\n",
    "        self.fc2 = nn.Linear(in_features=500, out_features=200)\n",
    "        self.fc3 = nn.Linear(in_features=200, out_features=33)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print(x.shape)\n",
    "        # 1 layer\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "#         print(x.shape)\n",
    "\n",
    "        # 2 layer\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "#         print(x.shape)\n",
    "        \n",
    "        # 3 layer\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "#         print(x.shape)\n",
    "        \n",
    "        # 4 layer\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "#         print(x.shape)\n",
    "        \n",
    "        # 5 layer\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
    "#         print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "#         print(x.shape)\n",
    "        # 6 layer\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5)\n",
    "\n",
    "        # 7 layer\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, p=0.5)\n",
    "        \n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ac74a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet_model = alexnet().to(device)\n",
    "optimizer = torch.optim.Adam(alexnet_model.parameters(), lr = cfg['lr'])\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.3, patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f27972ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 96, 55, 55]          34,944\n",
      "       BatchNorm2d-2           [-1, 96, 55, 55]             192\n",
      "            Conv2d-3          [-1, 256, 27, 27]         614,656\n",
      "       BatchNorm2d-4          [-1, 256, 27, 27]             512\n",
      "            Conv2d-5          [-1, 384, 13, 13]         885,120\n",
      "       BatchNorm2d-6          [-1, 384, 13, 13]             768\n",
      "            Conv2d-7          [-1, 384, 13, 13]       1,327,488\n",
      "       BatchNorm2d-8          [-1, 384, 13, 13]             768\n",
      "            Conv2d-9          [-1, 256, 13, 13]         884,992\n",
      "      BatchNorm2d-10          [-1, 256, 13, 13]             512\n",
      "           Linear-11                  [-1, 500]       4,608,500\n",
      "           Linear-12                  [-1, 200]         100,200\n",
      "           Linear-13                   [-1, 33]           6,633\n",
      "================================================================\n",
      "Total params: 8,465,285\n",
      "Trainable params: 8,465,285\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 9.93\n",
      "Params size (MB): 32.29\n",
      "Estimated Total Size (MB): 42.79\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(alexnet_model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf754892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model=alexnet_model, train_loader=train_loader,\n",
    "          optimizer=optimizer, lr_scheduler=lr_scheduler):\n",
    "\n",
    "    model.train()\n",
    "    for data, label in train_loader:\n",
    "        data, label = data.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        _, pred = torch.max(output, 1)\n",
    "        loss = F.cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7edf3a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model=alexnet_model, val_loader=val_loader,\n",
    "            optimizer=optimizer, lr_scheduler=lr_scheduler):\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in val_loader:\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            test_loss += F.cross_entropy(output, label, reduction='sum').item()\n",
    "            \n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            test_acc += pred.eq(label.view_as(pred)).sum().item()\n",
    "        \n",
    "    test_loss /= len(val_loader.dataset)\n",
    "    test_acc = 100*test_acc/len(val_loader.dataset)\n",
    "    \n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf81943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model = alexnet_model, train_loader=train_loader, val_loader =val_loader,\n",
    "               optimizer =optimizer, lr_scheduler = lr_scheduler , epochs = cfg['epochs']):\n",
    "    \n",
    "    best_acc = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    stop_num = 0\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        since = time.time()\n",
    "        train()\n",
    "        train_loss, train_acc = evaluate(model, train_loader)\n",
    "        val_loss, val_acc = evaluate(model, val_loader)\n",
    "        lr_scheduler.step(val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        if val_acc > best_acc :\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            stop_num = 0\n",
    "        else :\n",
    "            stop_num +=1\n",
    "        \n",
    "        end_time = time.time() - since\n",
    "        \n",
    "        print(f'--------------epoch : {i+1}번째,  lr :{current_lr}-----------')\n",
    "        print(f'train loss : {round(train_loss,6)},  acc : {round(train_acc,2)}%')\n",
    "        print(f'  val loss : {round(train_loss,6)},  acc : {round(val_acc,2)}%')\n",
    "        print(f'걸린시간 : {round(end_time,2)}초')\n",
    "        \n",
    "        if stop_num >=10:\n",
    "            print('조기종료')\n",
    "            break\n",
    "        \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b49e752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------epoch : 1번째,  lr :0.0005-----------\n",
      "train loss : 2.520105,  acc : 37.9%\n",
      "  val loss : 2.520105,  acc : 37.45%\n",
      "걸린시간 : 81.6초\n",
      "--------------epoch : 2번째,  lr :0.0005-----------\n",
      "train loss : 1.97252,  acc : 47.28%\n",
      "  val loss : 1.97252,  acc : 47.01%\n",
      "걸린시간 : 80.76초\n",
      "--------------epoch : 3번째,  lr :0.0005-----------\n",
      "train loss : 1.538666,  acc : 58.21%\n",
      "  val loss : 1.538666,  acc : 58.07%\n",
      "걸린시간 : 80.85초\n",
      "--------------epoch : 4번째,  lr :0.0005-----------\n",
      "train loss : 1.296329,  acc : 63.91%\n",
      "  val loss : 1.296329,  acc : 62.55%\n",
      "걸린시간 : 81.78초\n",
      "--------------epoch : 5번째,  lr :0.0005-----------\n",
      "train loss : 1.670957,  acc : 60.88%\n",
      "  val loss : 1.670957,  acc : 60.87%\n",
      "걸린시간 : 81.22초\n",
      "--------------epoch : 6번째,  lr :0.0005-----------\n",
      "train loss : 0.736026,  acc : 77.63%\n",
      "  val loss : 0.736026,  acc : 77.68%\n",
      "걸린시간 : 77.8초\n",
      "--------------epoch : 7번째,  lr :0.0005-----------\n",
      "train loss : 0.87222,  acc : 75.09%\n",
      "  val loss : 0.87222,  acc : 73.79%\n",
      "걸린시간 : 77.47초\n",
      "--------------epoch : 8번째,  lr :0.0005-----------\n",
      "train loss : 1.898799,  acc : 60.38%\n",
      "  val loss : 1.898799,  acc : 60.42%\n",
      "걸린시간 : 77.28초\n",
      "--------------epoch : 9번째,  lr :0.0005-----------\n",
      "train loss : 2.050331,  acc : 61.92%\n",
      "  val loss : 2.050331,  acc : 61.86%\n",
      "걸린시간 : 77.24초\n",
      "--------------epoch : 10번째,  lr :0.0005-----------\n",
      "train loss : 2.398137,  acc : 55.88%\n",
      "  val loss : 2.398137,  acc : 55.91%\n",
      "걸린시간 : 76.82초\n",
      "--------------epoch : 11번째,  lr :0.0005-----------\n",
      "train loss : 0.474524,  acc : 85.64%\n",
      "  val loss : 0.474524,  acc : 85.37%\n",
      "걸린시간 : 76.72초\n",
      "--------------epoch : 12번째,  lr :0.0005-----------\n",
      "train loss : 0.498367,  acc : 86.0%\n",
      "  val loss : 0.498367,  acc : 85.91%\n",
      "걸린시간 : 76.73초\n",
      "--------------epoch : 13번째,  lr :0.0005-----------\n",
      "train loss : 0.481708,  acc : 86.12%\n",
      "  val loss : 0.481708,  acc : 84.82%\n",
      "걸린시간 : 76.74초\n",
      "--------------epoch : 14번째,  lr :0.0005-----------\n",
      "train loss : 0.785465,  acc : 79.94%\n",
      "  val loss : 0.785465,  acc : 79.87%\n",
      "걸린시간 : 76.48초\n",
      "--------------epoch : 15번째,  lr :0.0005-----------\n",
      "train loss : 1.059478,  acc : 77.78%\n",
      "  val loss : 1.059478,  acc : 76.44%\n",
      "걸린시간 : 77.41초\n",
      "--------------epoch : 16번째,  lr :0.0005-----------\n",
      "train loss : 0.432209,  acc : 87.48%\n",
      "  val loss : 0.432209,  acc : 87.24%\n",
      "걸린시간 : 77.21초\n",
      "--------------epoch : 17번째,  lr :0.0005-----------\n",
      "train loss : 0.635887,  acc : 83.73%\n",
      "  val loss : 0.635887,  acc : 83.45%\n",
      "걸린시간 : 77.39초\n",
      "--------------epoch : 18번째,  lr :0.0005-----------\n",
      "train loss : 1.06539,  acc : 76.49%\n",
      "  val loss : 1.06539,  acc : 75.85%\n",
      "걸린시간 : 77.08초\n",
      "--------------epoch : 19번째,  lr :0.0005-----------\n",
      "train loss : 2.838096,  acc : 54.46%\n",
      "  val loss : 2.838096,  acc : 53.24%\n",
      "걸린시간 : 77.57초\n",
      "--------------epoch : 20번째,  lr :0.0005-----------\n",
      "train loss : 0.623307,  acc : 84.66%\n",
      "  val loss : 0.623307,  acc : 83.19%\n",
      "걸린시간 : 77.21초\n",
      "--------------epoch : 21번째,  lr :0.00015-----------\n",
      "train loss : 0.604575,  acc : 84.21%\n",
      "  val loss : 0.604575,  acc : 84.1%\n",
      "걸린시간 : 77.22초\n",
      "--------------epoch : 22번째,  lr :0.00015-----------\n",
      "train loss : 0.102336,  acc : 96.99%\n",
      "  val loss : 0.102336,  acc : 96.08%\n",
      "걸린시간 : 77.62초\n",
      "--------------epoch : 23번째,  lr :0.00015-----------\n",
      "train loss : 0.115443,  acc : 96.61%\n",
      "  val loss : 0.115443,  acc : 95.73%\n",
      "걸린시간 : 76.96초\n",
      "--------------epoch : 24번째,  lr :0.00015-----------\n",
      "train loss : 0.084387,  acc : 97.45%\n",
      "  val loss : 0.084387,  acc : 96.51%\n",
      "걸린시간 : 77.17초\n",
      "--------------epoch : 25번째,  lr :0.00015-----------\n",
      "train loss : 0.083719,  acc : 97.4%\n",
      "  val loss : 0.083719,  acc : 96.7%\n",
      "걸린시간 : 77.45초\n",
      "--------------epoch : 26번째,  lr :0.00015-----------\n",
      "train loss : 0.128603,  acc : 96.17%\n",
      "  val loss : 0.128603,  acc : 95.14%\n",
      "걸린시간 : 77.42초\n",
      "--------------epoch : 27번째,  lr :0.00015-----------\n",
      "train loss : 0.093117,  acc : 97.15%\n",
      "  val loss : 0.093117,  acc : 96.06%\n",
      "걸린시간 : 77.24초\n",
      "--------------epoch : 28번째,  lr :0.00015-----------\n",
      "train loss : 0.090923,  acc : 97.26%\n",
      "  val loss : 0.090923,  acc : 96.08%\n",
      "걸린시간 : 77.48초\n",
      "--------------epoch : 29번째,  lr :0.00015-----------\n",
      "train loss : 0.074128,  acc : 97.69%\n",
      "  val loss : 0.074128,  acc : 96.91%\n",
      "걸린시간 : 77.28초\n",
      "--------------epoch : 30번째,  lr :0.00015-----------\n",
      "train loss : 0.137123,  acc : 96.0%\n",
      "  val loss : 0.137123,  acc : 95.12%\n",
      "걸린시간 : 77.26초\n",
      "--------------epoch : 31번째,  lr :0.00015-----------\n",
      "train loss : 0.202152,  acc : 94.35%\n",
      "  val loss : 0.202152,  acc : 93.28%\n",
      "걸린시간 : 77.74초\n",
      "--------------epoch : 32번째,  lr :0.00015-----------\n",
      "train loss : 0.116901,  acc : 96.52%\n",
      "  val loss : 0.116901,  acc : 95.69%\n",
      "걸린시간 : 77.45초\n",
      "--------------epoch : 33번째,  lr :0.00015-----------\n",
      "train loss : 0.174397,  acc : 95.05%\n",
      "  val loss : 0.174397,  acc : 93.93%\n",
      "걸린시간 : 77.38초\n",
      "--------------epoch : 34번째,  lr :4.4999999999999996e-05-----------\n",
      "train loss : 0.123478,  acc : 96.47%\n",
      "  val loss : 0.123478,  acc : 95.34%\n",
      "걸린시간 : 77.48초\n",
      "--------------epoch : 35번째,  lr :4.4999999999999996e-05-----------\n",
      "train loss : 0.047233,  acc : 98.51%\n",
      "  val loss : 0.047233,  acc : 97.68%\n",
      "걸린시간 : 77.57초\n",
      "--------------epoch : 36번째,  lr :4.4999999999999996e-05-----------\n",
      "train loss : 0.054297,  acc : 98.37%\n",
      "  val loss : 0.054297,  acc : 97.12%\n",
      "걸린시간 : 77.29초\n",
      "--------------epoch : 37번째,  lr :4.4999999999999996e-05-----------\n",
      "train loss : 0.034651,  acc : 98.97%\n",
      "  val loss : 0.034651,  acc : 97.95%\n",
      "걸린시간 : 77.23초\n",
      "--------------epoch : 38번째,  lr :4.4999999999999996e-05-----------\n",
      "train loss : 0.037504,  acc : 98.82%\n",
      "  val loss : 0.037504,  acc : 97.91%\n",
      "걸린시간 : 77.41초\n",
      "--------------epoch : 39번째,  lr :4.4999999999999996e-05-----------\n",
      "train loss : 0.040716,  acc : 98.76%\n",
      "  val loss : 0.040716,  acc : 97.8%\n",
      "걸린시간 : 77.12초\n",
      "--------------epoch : 40번째,  lr :4.4999999999999996e-05-----------\n",
      "train loss : 0.048251,  acc : 98.53%\n",
      "  val loss : 0.048251,  acc : 97.31%\n",
      "걸린시간 : 77.48초\n",
      "--------------epoch : 41번째,  lr :4.4999999999999996e-05-----------\n",
      "train loss : 0.0327,  acc : 98.98%\n",
      "  val loss : 0.0327,  acc : 97.91%\n",
      "걸린시간 : 77.4초\n",
      "--------------epoch : 42번째,  lr :4.4999999999999996e-05-----------\n",
      "train loss : 0.03695,  acc : 98.87%\n",
      "  val loss : 0.03695,  acc : 98.17%\n",
      "걸린시간 : 76.09초\n",
      "--------------epoch : 43번째,  lr :4.4999999999999996e-05-----------\n",
      "train loss : 0.033491,  acc : 98.95%\n",
      "  val loss : 0.033491,  acc : 98.24%\n",
      "걸린시간 : 75.72초\n",
      "--------------epoch : 44번째,  lr :4.4999999999999996e-05-----------\n",
      "train loss : 0.0343,  acc : 98.99%\n",
      "  val loss : 0.0343,  acc : 98.19%\n",
      "걸린시간 : 75.28초\n",
      "--------------epoch : 45번째,  lr :4.4999999999999996e-05-----------\n",
      "train loss : 0.042964,  acc : 98.74%\n",
      "  val loss : 0.042964,  acc : 97.67%\n",
      "걸린시간 : 75.31초\n",
      "--------------epoch : 46번째,  lr :4.4999999999999996e-05-----------\n",
      "train loss : 0.039678,  acc : 98.77%\n",
      "  val loss : 0.039678,  acc : 97.88%\n",
      "걸린시간 : 75.45초\n",
      "--------------epoch : 47번째,  lr :4.4999999999999996e-05-----------\n",
      "train loss : 0.030424,  acc : 99.04%\n",
      "  val loss : 0.030424,  acc : 98.13%\n",
      "걸린시간 : 75.8초\n",
      "--------------epoch : 48번째,  lr :1.3499999999999998e-05-----------\n",
      "train loss : 0.037807,  acc : 98.78%\n",
      "  val loss : 0.037807,  acc : 97.58%\n",
      "걸린시간 : 75.57초\n",
      "--------------epoch : 49번째,  lr :1.3499999999999998e-05-----------\n",
      "train loss : 0.026502,  acc : 99.21%\n",
      "  val loss : 0.026502,  acc : 98.39%\n",
      "걸린시간 : 75.2초\n",
      "--------------epoch : 50번째,  lr :1.3499999999999998e-05-----------\n",
      "train loss : 0.025635,  acc : 99.24%\n",
      "  val loss : 0.025635,  acc : 98.26%\n",
      "걸린시간 : 77.17초\n",
      "--------------epoch : 51번째,  lr :1.3499999999999998e-05-----------\n",
      "train loss : 0.024718,  acc : 99.29%\n",
      "  val loss : 0.024718,  acc : 98.44%\n",
      "걸린시간 : 75.38초\n",
      "--------------epoch : 52번째,  lr :1.3499999999999998e-05-----------\n",
      "train loss : 0.024263,  acc : 99.27%\n",
      "  val loss : 0.024263,  acc : 98.42%\n",
      "걸린시간 : 75.01초\n",
      "--------------epoch : 53번째,  lr :1.3499999999999998e-05-----------\n",
      "train loss : 0.021396,  acc : 99.37%\n",
      "  val loss : 0.021396,  acc : 98.5%\n",
      "걸린시간 : 75.3초\n",
      "--------------epoch : 54번째,  lr :1.3499999999999998e-05-----------\n",
      "train loss : 0.023085,  acc : 99.31%\n",
      "  val loss : 0.023085,  acc : 98.29%\n",
      "걸린시간 : 75.15초\n",
      "--------------epoch : 55번째,  lr :1.3499999999999998e-05-----------\n",
      "train loss : 0.024713,  acc : 99.24%\n",
      "  val loss : 0.024713,  acc : 98.47%\n",
      "걸린시간 : 76.06초\n",
      "--------------epoch : 56번째,  lr :4.049999999999999e-06-----------\n",
      "train loss : 0.024583,  acc : 99.32%\n",
      "  val loss : 0.024583,  acc : 98.31%\n",
      "걸린시간 : 75.36초\n",
      "--------------epoch : 57번째,  lr :4.049999999999999e-06-----------\n",
      "train loss : 0.022479,  acc : 99.36%\n",
      "  val loss : 0.022479,  acc : 98.62%\n",
      "걸린시간 : 75.22초\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------epoch : 58번째,  lr :4.049999999999999e-06-----------\n",
      "train loss : 0.022839,  acc : 99.31%\n",
      "  val loss : 0.022839,  acc : 98.4%\n",
      "걸린시간 : 75.16초\n",
      "--------------epoch : 59번째,  lr :4.049999999999999e-06-----------\n",
      "train loss : 0.022323,  acc : 99.26%\n",
      "  val loss : 0.022323,  acc : 98.62%\n",
      "걸린시간 : 75.32초\n",
      "--------------epoch : 60번째,  lr :4.049999999999999e-06-----------\n",
      "train loss : 0.023145,  acc : 99.29%\n",
      "  val loss : 0.023145,  acc : 98.27%\n",
      "걸린시간 : 75.21초\n",
      "--------------epoch : 61번째,  lr :4.049999999999999e-06-----------\n",
      "train loss : 0.022843,  acc : 99.3%\n",
      "  val loss : 0.022843,  acc : 98.54%\n",
      "걸린시간 : 75.39초\n",
      "--------------epoch : 62번째,  lr :1.2149999999999998e-06-----------\n",
      "train loss : 0.02148,  acc : 99.4%\n",
      "  val loss : 0.02148,  acc : 98.46%\n",
      "걸린시간 : 75.0초\n",
      "--------------epoch : 63번째,  lr :1.2149999999999998e-06-----------\n",
      "train loss : 0.021597,  acc : 99.31%\n",
      "  val loss : 0.021597,  acc : 98.32%\n",
      "걸린시간 : 74.65초\n",
      "--------------epoch : 64번째,  lr :1.2149999999999998e-06-----------\n",
      "train loss : 0.021863,  acc : 99.37%\n",
      "  val loss : 0.021863,  acc : 98.46%\n",
      "걸린시간 : 74.97초\n",
      "--------------epoch : 65번째,  lr :1.2149999999999998e-06-----------\n",
      "train loss : 0.021608,  acc : 99.28%\n",
      "  val loss : 0.021608,  acc : 98.45%\n",
      "걸린시간 : 74.31초\n",
      "--------------epoch : 66번째,  lr :1.2149999999999998e-06-----------\n",
      "train loss : 0.021487,  acc : 99.36%\n",
      "  val loss : 0.021487,  acc : 98.52%\n",
      "걸린시간 : 74.39초\n",
      "--------------epoch : 67번째,  lr :3.644999999999999e-07-----------\n",
      "train loss : 0.022611,  acc : 99.29%\n",
      "  val loss : 0.022611,  acc : 98.56%\n",
      "걸린시간 : 74.63초\n",
      "조기종료\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1370c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(trained_model, './mymodel.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3f8a27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
